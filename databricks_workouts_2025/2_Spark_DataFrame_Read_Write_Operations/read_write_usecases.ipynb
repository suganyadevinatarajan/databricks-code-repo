{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba86a20-5a3a-4130-86f5-e312f4a7901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain ReadOps Assignment\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2766646-7223-43da-9293-5805c95f9e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign;\n",
    "create schema if not exists telecom_catalog_assign.landing_zone ;\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf2fe69e-4278-47a2-a034-23ca08dcfe82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93eac91c-06a6-455c-87f3-79ef2e4e16b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### a. Volume vs DBFS \n",
    "\n",
    "**Volumes** are Unity Catalog-governed objects that provide a logical layer for managing non-tabular data (files) within cloud object storage with centralized governance. They are the modern, recommended approach in Databricks and support access control policies and lineage tracking through Unity Catalog.  \n",
    "**DBFS/FileStore** (Databricks File System) is an older abstraction over cloud storage that allowed users to interact with data using simple paths or mounts without robust, centralized governance. The DBFS root and its mounts are deprecated, and Databricks recommends migrating to volumes or external locations under Unity Catalog.\n",
    "\n",
    "\n",
    "| Feature                | Volumes (Unity Catalog)                                                                 | DBFS/FileStore (Deprecated)                                  |\n",
    "|------------------------|----------------------------------------------------------------------------------------|--------------------------------------------------------------|\n",
    "| Governance             | Centralized, fine-grained access control via Unity Catalog                             | Limited, workspace-level controls                            |\n",
    "| Data Lineage           | Supported                                                                              | Not supported                                                |\n",
    "| Recommended for Prod   | Yes                                                                                    | No                                                           |\n",
    "| Access Control         | Unity Catalog policies                                                                 | ACLs, less granular                                          |\n",
    "| Usage                  | Non-tabular data, files, ML models, etc.                                               | General file storage                                         |\n",
    "| Migration              | Modern, recommended approach                                                           | Deprecated, migrate to Volumes or External Locations         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5e0fbc6-648e-4f4b-a2a4-54967df480ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###b. Why production teams prefer Volumes for regulated data\n",
    "\n",
    "\n",
    "Production teams prefer Volumes for regulated data primarily due to their integration with Unity Catalog, which provides a robust framework for data governance, security, and access management. \n",
    "\n",
    "**Centralized Governance and Auditing:** Unity Catalog provides a single place to manage data access, permissions, and auditing across all Databricks workspaces. This is essential for meeting compliance requirements for regulated data.\n",
    "\n",
    "**Granular Access Control:** Volumes allow administrators to define precise access controls on specific volumes or subfolders within the cloud storage location, which is critical for restricted and sensitive data.\n",
    "\n",
    "**Simplified Compliance:** The structured, governed approach of volumes simplifies compliance reporting and ensures data is handled consistently according to organizational policies and industry regulations.\n",
    "\n",
    "**Lifecycle Management:** Volumes have their own policies for permissions, encryption, backup, and recovery, which aids in managing the full data lifecycle in a compliant manner. \n",
    "\n",
    "**DBFS** lacks these centralized governance features, making it difficult to enforce consistent, auditable access controls required for sensitive or regulated production data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d239288e-3a41-4de6-8658-9090955691b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Loading the datasets into Volume\n",
    "\n",
    "1. Write code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3654af-8ef0-4034-a2b9-dc9a58027772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Loading the customer data into customer.csv file\n",
    "\n",
    "customer_csv = '''101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,\n",
    "104,Raj,52,Mumbai,\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID '''\n",
    "dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",customer_csv,overwrite=True)\n",
    "\n",
    "#Loading the usage data into usage.tsv file\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",usage_tsv,overwrite=True)\n",
    "\n",
    "#Loading the tower log data into regionwise .tsv file\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''\n",
    "dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1/tower_logs_region1.csv\",tower_logs_region1,overwrite=True)\n",
    "dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2/tower_logs_region2.csv\",tower_logs_region1,overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb2dca0-cb14-414d-8bd1-f404539f2208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4dafe9-d3dd-48b6-914b-c7c25340af20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Verify if data is loaded\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82b08008-0c45-444a-a227-35dee2f5f1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#validating if customer file is loaded successfully\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.head(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e461bd-882f-4823-9606-3bfd79178745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#validating if usage file is loaded successfully\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.head(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afed1170-cce6-4b0b-abd8-91797d1267c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#validating if tower log file is loaded successfully\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1/\")\n",
    "dbutils.fs.head(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1/tower_logs_region1.csv\")\n",
    "\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2/\")\n",
    "dbutils.fs.head(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2/tower_logs_region2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Directory Read Use Cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "542af18c-e005-45fe-97cf-c8c8832fd96c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.1. Read all tower logs using: Path glob filter (example: *.csv) Multiple paths input Recursive lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9ec7fd-96b0-48cd-8572-f16c91d462cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pth_glb_fl_df1=spark.read.csv(path=[\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1\",\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2\"],header=True,inferSchema=True,sep='|',pathGlobFilter=\"*.csv\",recursiveFileLookup=True)\n",
    "pth_glb_fl_df1.show()\n",
    "display(pth_glb_fl_df1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ad9629-481f-4fe0-8355-e8c5a3d05f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d49c48d-bd54-448c-8b2b-9fd6fad90bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using pathGlobFilter\n",
    "\n",
    "pth_glb_fl_df1=spark.read.csv(path=[\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1\",\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2\"],header=True,inferSchema=True,sep='|',pathGlobFilter=\"*.csv\",recursiveFileLookup=True)\n",
    "pth_glb_fl_df1.show()\n",
    "display(pth_glb_fl_df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5116f156-897b-41c8-ae11-e68663548ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using list of paths in spark.read.csv([path1, path2]) \n",
    "\n",
    "mul_pth_df1=spark.read.csv(path:=[\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1/\",\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2/\"],header=True,inferSchema=True, sep='|',recursiveFileLookup=True)\n",
    "mul_pth_df1.show()\n",
    "display(mul_pth_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b997b351-1f41-41d1-b838-5a270f009329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "mul_pth_df2= spark.read.option(\"header\",True).option(\"delimiter\",\"|\").option(\"recursiveFileLookup\",True).format(\"csv\").load([\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\"])\n",
    "mul_pth_df2.show()\n",
    "display(mul_pth_df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b96d73d-aff7-41e7-bcc9-446f45f3c140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.3 Compare the outputs and understand when each should be used.\n",
    "\n",
    "- pathGlobFilter=\"*.csv\" -This can be used to read files of specific(*.csv) format within the specifed path/folder\n",
    "- List of paths -This can be used to read files from multiple paths/sources\n",
    "- Option -This can be used for more than one options, and parameters can be passed to the option & recursiveFileLookup can be used to read files from all the sub folders in the mentioned path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9282bbe5-f429-4847-b8e0-05b77dc16779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###4.1.a Try the Customer file with the option and options using read.csv and format function:\n",
    "header=false, inferSchema=false\n",
    "or\n",
    "header=true, inferSchema=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b1ee4c77-28d0-46d8-8774-78b7cfe1eb89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Read cust file with option and header=true, inferSchema=true\n",
    "cust_df1=spark.read.option(\"header\",True).option(\"inferSchema\",True).option(\"delimiter\",\",\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df1)\n",
    "print(cust_df1.schema)\n",
    "\n",
    "##Read cust file with options and header=true, inferSchema=true\n",
    "cust_df2=spark.read.options(header=True,inferSchema=True,delimiter=\",\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df2)\n",
    "print(cust_df2.schema)\n",
    "\n",
    "#Read cust file with options.csv and header=true, inferSchema=true\n",
    "cust_df3=spark.read.options(header=True,inferSchema=True,delimiter=\",\").csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df3)\n",
    "print(cust_df3.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b9617a90-17c3-4380-bd58-7c93f177411d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##Read cust file with option and header=false, inferSchema=false\n",
    "cust_df4=spark.read.option(\"header\",False).option(\"inferSchema\",False).option(\"delimiter\",\",\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df4)\n",
    "print(cust_df4.schema)\n",
    "\n",
    "##Read cust file with options and header=false, inferSchema=false\n",
    "cust_df5=spark.read.options(header=False,inferSchema=False,delimiter=\",\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df5)\n",
    "print(cust_df5.schema)\n",
    "\n",
    "#Read cust file with options.csv and header=False, inferSchema=false\n",
    "cust_df6=spark.read.options(header=False,inferSchema=False,delimiter=\",\").csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df6)\n",
    "print(cust_df6.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "203e3f58-cfce-4ec7-8bb9-fb1e82917ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###4.1.b Try the Usage files with the option and options using read.csv and format function:\n",
    "header=false, inferSchema=false\n",
    "or\n",
    "header=true, inferSchema=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c19db0-24f9-4500-a035-03908873cc6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Read usage file with option and header=True, inferSchema=True\n",
    "usage_df1=spark.read.option(\"header\",True).option(\"inferSchema\",True).option(\"delimiter\",\"\\t\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df1)\n",
    "print(usage_df1.schema)\n",
    "\n",
    "##Read usage file with options and header=True, inferSchema=True\n",
    "usage_df2=spark.read.options(header=True,inferSchema=True,delimiter=\"\\t\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df2)\n",
    "print(usage_df2.schema)\n",
    "\n",
    "##Read usage file with options.csv and header=True, inferSchema=True\n",
    "usage_df3=spark.read.options(header=True,inferSchema=True,delimiter=\"\\t\").csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df3)\n",
    "print(usage_df3.schema) \n",
    "\n",
    "##Read usage file with option and header=False, inferSchema=False\n",
    "usage_df4=spark.read.option(\"header\",False).option(\"inferSchema\",False).option(\"delimiter\",\"\\t\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df4)\n",
    "print(usage_df4.schema)\n",
    "\n",
    "##Read usage file with options and header=False, inferSchema=False\n",
    "usage_df5=spark.read.options(header=False,inferSchema=False,delimiter=\"\\t\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df5)\n",
    "print(usage_df5.schema)\n",
    "\n",
    "##Read usage file with options.csv and header=False, inferSchema=False\n",
    "usage_df6=spark.read.options(header=False,inferSchema=False,delimiter=\"\\t\").csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df6)\n",
    "print(usage_df6.schema) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb76bc66-b4e3-4e10-9f47-a92c6cc90d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write a note on What changed when we use header or inferSchema with true/false?\n",
    "\n",
    "- **Header = True** - When header is assigned as True its considering the first row as header\n",
    "- **Header = False** - When header is assigned as False its assigning header as c0, c1, c2..etc.,\n",
    "- **InferSchema = True** - When inferSchema is assigned as True it scans all rows and assigns datatype accordingly. It can be used in small datasets, but in huge volume of data full scan takes time and needs to be used carefully. Sampling ratio option can be used to avoid full scanning.\n",
    "- **InferSchema = False** -When inferSchema is assigned as False all the columns are treated as strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67afb6a0-951a-4bbe-835a-c6e645401a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How schema inference handled “abc” in age?\n",
    "\n",
    "The column age has one value as \"abc\", even when InferSchema is True its assigned as string datatype, to be on safer side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ff42c77-a068-4302-a496-ffbba95ddce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5.1 Apply column names using string using toDF function for customer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "120611e3-a3ff-465a-92e9-e052814c065d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df=spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",header=True,inferSchema=True).toDF(\"customer_id\",\"name\",\"age\",\"city\",\"plan\")\n",
    "display(cust_df)\n",
    "print(cust_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae348339-125a-4424-b097-616151443410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5.2 Apply column names and datatype using the schema function for usage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47d7d6e-9ead-4f3d-afd6-498b5c9772cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "str_struct= \"customer_id integer,voice_mins integer,data_mb integer,sms_count integer\"\n",
    "usage_df=spark.read.schema(str_struct).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",header=True,sep=\"\\t\")\n",
    "display(usage_df)\n",
    "print(usage_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d68caf16-4bc5-4289-9ebd-5777cc5afee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5.3 Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b68da42-3b86-4dab-ad1b-82a4e97127ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"id\",IntegerType(),False),StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"event_id\",IntegerType(),True),StructField(\"customer_id\",IntegerType(),True),StructField(\"tower_id\",StringType(),True),StructField(\"signal_strength\",IntegerType(),True),StructField(\"timestamp\",StringType())])\n",
    "usage_df_df=spark.read.schema(custom_schema).csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/sourcedata/custs_header_1\")\n",
    "print(usage_df.printSchema())\n",
    "usage_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b51df02-195c-4bc8-836e-facc13a9ba59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Write Operations using \n",
    "- csv, json, orc, parquet, delta, saveAsTable, insertInto, xml with different write mode, header and sep options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e5949a2-9fec-4560-a4e9-9e0966287e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Write Operations (Data Conversion/Schema migration) – CSV Format Usecases\n",
    "1. Write customer data into CSV format using overwrite mode\n",
    "2. Write usage data into CSV format using append mode\n",
    "3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34cd3467-a1e0-4b8b-a577-e483a3311eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6.1 Write customer data into CSV format using overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13de3ba3-738f-4129-a63d-0aa3dcf530a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wr_csv_df=cust_df1.write.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/csvout\",mode=\"overwrite\",header=True,sep='~')  \n",
    "spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/csvout\",header=True,sep='~').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6612a3d8-ca49-408a-8fcd-978fc3316cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6.2 Write usage data into CSV format using append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e08fee-95bb-45e1-9dc2-81c877a94e7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usg_wr_df=usage_df1.write.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_csvout\",mode=\"append\",header=True,sep='\\t')\n",
    "spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_csvout\",header=True,sep='\\t').show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e33ca88-30d0-46cb-8d1d-6353eec6ba05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6.3 Write tower data into CSV format with header enabled and custom separator (|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243eb73f-b20b-4471-87e3-26e7ed6c0b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mul_pth_df2.write.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/tower_out\",mode=\"overwrite\",header=True,sep='|')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab96825c-c71e-4499-926f-38dff71ed5a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6.4 Read the tower data in a dataframe and show only 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "944f3e1e-1e66-4623-9d14-835281c82c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_csv_out_df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/tower_out\",header=True,sep='|').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef94c17f-9944-42f4-a782-e642e9df1db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6.5 Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e22e9d5a-479c-46cf-9be1-47b40f4f6e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below is the csv data downloaded and opened in notepad++, the data is in structured and readble format\n",
    "\n",
    "event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "789ce68e-76bb-4cb9-a6b5-f044f4293cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Write Operations (Data Conversion/Schema migration)– JSON Format Usecases\n",
    "1. Write customer data into JSON format using overwrite mode\n",
    "2. Write usage data into JSON format using append mode and snappy compression format\n",
    "3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fbf7c47-d7b6-4ab2-b335-8b8201e55710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###7.1 Write customer data into JSON format using overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9a7f4f8a-90c9-4d3e-911b-4493037ae8f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wr_json_df1=cust_df1.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_json_out\",mode=\"overwrite\")\n",
    "spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_json_out\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ee2e89-acbf-406f-9e4f-55e1b415c406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###7.2 Write usage data into JSON format using append mode and snappy compression format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a9346e-08e7-4b4d-bbdf-265b86fdfecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df1.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usage_json_out\",mode=\"append\",compression=\"snappy\")\n",
    "spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usage_json_out\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbcdf903-521b-4129-af5b-d9c27218afa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###7.3 Write tower data into JSON format using ignore mode and observe the behavior of this mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7472abc9-ae4c-494a-8e54-03456118d354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mul_pth_df2.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/mul_pth_df2\",mode=\"ignore\")\n",
    "#since the directory mul_pth_df2 is already present in target folder the write operation is ignored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cfa634b-2c95-46d5-8472-eb6273c6d761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###7.4 Read the tower data in a dataframe and show only 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f43d30-a258-4a1c-a788-1b5530b14966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "twr_json_df=spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/mul_pth_df2\")\n",
    "twr_json_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d6b7364-fb3f-462b-8ab2-435e5bd50410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###7.5 Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ad5b62-d536-4e56-be88-dd54f45b3990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below is the json format downloaded and opened in notepad++, data is in semistructured dictonary format (key: value pair)\n",
    "{\"event_id\":\"5001\",\"customer_id\":\"101\",\"tower_id\":\"TWR01\",\"signal_strength\":\"-80\",\"timestamp\":\"2025-01-10 10:21:54\"}\n",
    "{\"event_id\":\"5004\",\"customer_id\":\"104\",\"tower_id\":\"TWR05\",\"signal_strength\":\"-75\",\"timestamp\":\"2025-01-10 11:01:12\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a637aaf-5292-4540-9b8f-3def1dbb7d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8. Write Operations (Data Conversion/Schema migration) – Parquet Format Usecases\n",
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "2. Write usage data into Parquet format using error mode\n",
    "3. Write tower data into Parquet format with gzip compression option\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9520a4cf-6f4a-4cde-b13e-34cd57d17662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###8.1 Write customer data into Parquet format using overwrite mode and in a gzip format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a955dd0-03d1-4bb1-b33e-fa67ea939b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_parquet_out\",mode=\"overwrite\",compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f86e6b0-8e2b-45e3-9828-492b3427c34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###8.2 Write usage data into Parquet format using error mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e9d303-91c3-4351-9590-09942e207e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#below is alternative way of writing into parquet file \n",
    "usage_df1.write.mode(\"error\").option(\"compression\",\"gzip\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_parquet_out/\")\n",
    "\n",
    "#Error message is received since the directory exists already: [PATH_ALREADY_EXISTS] Path dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_parquet_out already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14b04b67-3763-46e4-97f9-8eefdb7a0206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###8.3 Write tower data into Parquet format with gzip compression option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7a39046-4df9-4696-9dac-ddd798ca948c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mul_pth_df2.write.mode(\"append\").option(\"compression\",\"gzip\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/twr_parquet_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f1fabe-c697-4003-90f7-4b6aa4eff1b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###8.4 Read the usage data in a dataframe and show only 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7fc393a-61ad-4726-9ba9-7fc429239918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usg_wr_op_df=spark.read.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_parquet_out\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49d85f13-e09e-4ff2-9b15-08668cabd364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###8.5 Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3d14770-f887-4050-978b-007c818887fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below is the parquet data file which is structured & nested format \n",
    "\n",
    "PAR1\u0015 \u0015,\u0015F\u0015š˜‹œ\u0007\u001C\u0015\u0004\u0015 \u0015\u0006\u0015\b  \u001F‹\b      \u0003cb```ff\u0001’¦\u0006\u0006†PÚ\u0004 \u001FÏ+‡\u0016   \u0015 \u0015(\u0015D\u0015Óád\u001C\u0015\u0004\u0015 \u0015\u0006\u0015\b  \u001F‹\b      \u0003cb```ff\u0006’†\u0006†\u0010Ê\u0004 \u001A;\u001B‹\u0014   \u0015 \u00150\u0015H\u0015›œå\u001C\u0015\u0004\u0015 \u0015\u0006\u0015\b  \u001F‹\b      \u0003cb```ff\u0005’!áA\u0006†0†) Ü¬©\u0018   \u0015 \u0015(\u0015F\u0015 ¾ä#\u001C\u0015\u0004\u0015 \u0015\u0006\u0015\b  \u001F‹\b      \u0003cb```ff\u0006’º\u0016\u0006`ÊÜ\u0014 ¿\u0004ì\u0014   \u0015 \u0015h\u0015p\u0015«±»\u0005\u001C\u0015\u0004\u0015 \u0015\u0006\u0015\b  \u001F‹\b      \u0003cb```f\u0016\u0006’F\u0006F¦º\u0006†º†\u0006\n",
    "†\u0006VF†V¦&èÂ†V\u0006†V†F ´\u0002ÿ™4   \u0019\u0011\u0002\u0019\u0018\u00045001\u0019\u0018\u00045004\u0015\u0002\u0019\u0016 \u0019\u0016\u0004\u0019& \u0004 \u0019\u0011\u0002\u0019\u0018\u0003101\u0019\u0018\u0003104\u0015\u0002\u0019\u0016 \u0019\u0016\u0004\u0019& \u0004 \u0019\u0011\u0002\u0019\u0018\u0005TWR01\u0019\u0018\u0005TWR05\u0015\u0002\u0019\u0016 \u0019\u0016\u0004\u0019& \u0004 \u0019\u0011\u0002\u0019\u0018\u0003-75\u0019\u0018\u0003-80\u0015\u0002\u0019\u0016 \u0019\u0016\u0004\u0019& \u0004 \u0019\u0011\u0002\u0019\u0018\u00132025-01-10 10:21:54\u0019\u0018\u00132025-01-10 11:01:12\u0015\u0002\u0019\u0016 \u0019\u0016\u0004\u0019& \u0004 \u0019\u001C\u0016\b\u0015t\u0016  \u0019\u0016\u0010 \u0019\u001C\u0016|\u0015p\u0016  \u0019\u0016\f \u0019\u001C\u0016ì\u0001\u0015t\u0016  \u0019\u0016\u0014 \u0019\u001C\u0016à\u0002\u0015r\u0016  \u0019\u0016\f \u0019\u001C\u0016Ò\u0003\u0015ž\u0001\u0016  \u0019\u0016L \u0015\u0002\u0019lH\fspark_schema\u0015\n",
    " \u0015\f%\u0002\u0018\bevent_id% L\u001C   \u0015\f%\u0002\u0018\u000Bcustomer_id% L\u001C   \u0015\f%\u0002\u0018\btower_id% L\u001C   \u0015\f%\u0002\u0018\u000Fsignal_strength% L\u001C   \u0015\f%\u0002\u0018\ttimestamp% L\u001C   \u0016\u0004\u0019\u001C\u0019\\& \u001C\u0015\f\u00195\b\u0006 \u0019\u0018\bevent_id\u0015\u0004\u0016\u0004\u0016Z\u0016t&\b<6 (\u00045004\u0018\u00045001 \u0019\u001C\u0015 \u0015 \u0015\u0002 <\u0016\u0010\u0019\u0016\u0004\u0019& \u0004  \u0016Ô\u0007\u0015\u001A\u0016ð\u0004\u0015< & \u001C\u0015\f\u00195\b\u0006 \u0019\u0018\u000Bcustomer_id\u0015\u0004\u0016\u0004\u0016T\u0016p&|<6 (\u0003104\u0018\u0003101 \u0019\u001C\u0015 \u0015 \u0015\u0002 <\u0016\f\u0019\u0016\u0004\u0019& \u0004  \u0016î\u0007\u0015\u001A\u0016¬\u0005\u00158 & \u001C\u0015\f\u00195\b\u0006 \u0019\u0018\btower_id\u0015\u0004\u0016\u0004\u0016\\\u0016t&ì\u0001<6 (\u0005TWR05\u0018\u0005TWR01 \u0019\u001C\u0015 \u0015 \u0015\u0002 <\u0016\u0014\u0019\u0016\u0004\u0019& \u0004  \u0016ˆ\b\u0015\u001C\u0016ä\u0005\u0015@ & \u001C\u0015\f\u00195\b\u0006 \u0019\u0018\u000Fsignal_strength\u0015\u0004\u0016\u0004\u0016T\u0016r&à\u0002<6 (\u0003-80\u0018\u0003-75 \u0019\u001C\u0015 \u0015 \u0015\u0002 <\u0016\f\u0019\u0016\u0004\u0019& \u0004  \u0016¤\b\u0015\u001C\u0016¤\u0006\u00158 & \u001C\u0015\f\u00195\b\u0006 \u0019\u0018\ttimestamp\u0015\u0004\u0016\u0004\u0016–\u0001\u0016ž\u0001&Ò\u0003<6 (\u00132025-01-10 11:01:12\u0018\u00132025-01-10 10:21:54 \u0019\u001C\u0015 \u0015 \u0015\u0002 <\u0016L\u0019\u0016\u0004\u0019& \u0004  \u0016À\b\u0015\u001E\u0016Ü\u0006\u0015x \u0016ô\u0003\u0016\u0004&\b\u0016è\u0004\u0014  \u0019L\u0018\u0018org.apache.spark.version\u0018\u00054.0.0 \u0018)org.apache.spark.sql.parquet.row.metadata\u0018ñ\u0002{\"type\":\"struct\",\"fields\":[{\"name\":\"event_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"customer_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"tower_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"signal_strength\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"timestamp\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]} \u0018\u001Fcom.databricks.spark.jobGroupId\u0018B1765960297983_8830991696410954426_432d8485349f4e54b11746c9cf0b26ad \u0018\u001Ecom.databricks.spark.clusterId\u0018\u00181217-083245-9frgsgtv-v2n \u0018Zparquet-mr version 1.15.1-databricks-0001 (build c7257b8faff5699e13bbc781679dc03f48c1102a)\u0019\\\u001C  \u001C  \u001C  \u001C  \u001C   &\u0005  PAR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aefbc064-e9a6-4340-85f6-6a1e1c202174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0489316f-3320-4833-9a82-b7799e70fa26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##9. Write Operations (Data Conversion/Schema migration) – Orc Format Usecases\n",
    "1. Write customer data into ORC format using overwrite mode\n",
    "2. Write usage data into ORC format using append mode\n",
    "3. Write tower data into ORC format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28640d86-db32-4145-b987-9e4f8d0dcf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###9.1 Write customer data into ORC format using overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71258e47-c438-4fcd-8d9e-b7620d2547e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df1.write.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_orc_out\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e87590-5a8c-4a7d-945c-8a9a942ce871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###9.2 Write usage data into ORC format using append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f703b4-1b0a-4f3d-af14-2aa6da73373e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df1.write.orc(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usage_orc_out\",mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df09542f-df66-4092-bf3c-2f49681ae600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###9.3 Write tower data into ORC format and see the output file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e003b124-35ec-4b40-a3b6-058133030d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mul_pth_df2.write.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/twr_orc_out\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d5c233-c1a2-45b8-b150-ad7c09370f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###9.4 Read the usage data in a dataframe and show only 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e92945-58d5-4064-9903-55ccccb03b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "us_orc_df=spark.read.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usage_orc_out\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc65241f-f5b4-42d5-bc67-2d71f5157403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###9.5 Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a16dcb3-7487-4a4a-915c-2a1c151310c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Below is the orc format of data downloaded in notepad++, the data is in structured and stripped format\n",
    "ORC\u0011  \n",
    "\u0006\u0012\u0004\b\u0002P ?  \n",
    "\u001D\n",
    "\u0005     \u0012\u0014\b\u0002\"\u000E\n",
    "\u00045001\u0012\u00045004\u0018\u0010P ;  \n",
    "\u001B\n",
    "\u0005     \u0012\u0012\b\u0002\"\f\n",
    "\u0003101\u0012\u0003104\u0018\fP C  \n",
    "\u001F\n",
    "\u0005     \u0012\u0016\b\u0002\"\u0010\n",
    "\u0005TWR01\u0012\u0005TWR05\u0018\u0014P ;  \n",
    "\u001B\n",
    "\u0005     \u0012\u0012\b\u0002\"\f\n",
    "\u0003-75\u0012\u0003-80\u0018\fP j  =\u0010\n",
    ";\n",
    "\u0005 \u0001\u0001l\u00122\b\u0002\",\n",
    "\u00132025-01-10 10:21:54\u00122\u0015 (1:01:12\u0018LP \u0011  50015004\u0007  F\u0001D\n",
    "  101104\u0007  B\u0001ð\u0015  TWR01TWR05\u0007  F\u0001U\n",
    "  -80-75\u0007  B\u0001ð@  &H2025-01-10 10:21:54.\u0013 \u00181:01:12\t  N\u0001\u0013\u0013Ô  ˜\u0001\u001C\n",
    "\u0006\b\u0006\u0010 \u0018\u000B\u0005\b\b\u0001\u0018\"\u0005\b\b\u0002\u0018 \u0005\b\b\u0003\u0018$\u0005\b \u0004\n",
    "\u0010 \u0005\u00188\n",
    "\u0006\b\u0001\u0010\u0001\u00050\u0010\u0002\u0010\u0001\u0018\u0006\u0005\u0010 \u0002\u0018\t\n",
    "\u0006\b\u0002\u0010\u0002\n",
    "\u0010\b\u0003\u0018\n",
    "\u0005\u0010 \u0003\n",
    "\u0010 \u0004\n",
    "  \u0004\n",
    "\u0010\b\u0005\u0018#\u0005 (\u0005\u0018\u0007\u0012\u0002\b \u0012\u0002\b\u0002>\u0004 \u001E\u0001 \u0001T\n",
    "š\u0001\n",
    "\u0004\b\u0002P \n",
    "\u0016\b\u0002\"\u000E\n",
    "\u00045001\u0012\u0001\u0006à4\u0018\u0010P X\u0011\n",
    "\u0014\b\u0002\"\f\n",
    "\u0003101\u0012\u0003104\u0018\fP X\u000F\n",
    "\u0018\b\u0002\"\u0010\n",
    "\u0005TWR01\u0012\u0005TWR05\u0018\u0014P X\u0013\n",
    "\u0014\t0\u001C-75\u0012\u0003-80\n",
    "0h4\b\u0002\",\n",
    "\u00132025-01-10 10:21:54\u00122\u0015 01:01:12\u0018LP X*ˆ\u0002 ê\u0003à\b\u0003\u0010¥\u0003\u001A\u000B\b\u0003\u0010É\u0001\u0018l m(\u0002\"F\b\f\u0012\u0005\u0001\u0002\u0003\u0004\u0005\u001A\bevent_id\u001A\u000Bcustomer_id\u001A\btow\t\n",
    "ðB\u000Fsignal_strength\u001A\ttimestamp\"%\b\u0007:!\n",
    "\u0017spark.sql.catalyst.type\u0012\u0006string\"þ' þ' j' 8*!\n",
    "\u0018org.apache.\tÊˆversion\u0012\u00054.0.00\u0002:\u0004\b\u0002P :\u0016\b\u0002\"\u000E\n",
    "\u00045001\u0012\u0001\u0006Ø4\u0018\u0010P X\u0011:\u0014\b\u0002\"\f\n",
    "\u0003101\u0012\u0003104\u0018\fP X\u000F:\u0018\b\u0002\"\u0010\n",
    "\u0005TWR01\u0012\u0005TWR05\u0018\u0014P X\u0013\u00110\u001C-75\u0012\u0003-80\n",
    "0h4\b\u0002\",\n",
    "\u00132025-01-10 10:21:54\u00122\u0015 h1:01:12\u0018LP X*@NH X\u0001b\u00052.1.1\bÇ\u0002\u0010\u0002\u0018€€\u0010\"\u0002 \f(’\u00010\t‚ô\u0003\u0003ORC\u0019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "045d3aac-97cb-4ccc-b0b5-b64e9b808a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##10. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases\n",
    "1. Write customer data into Delta format using overwrite mode\n",
    "2. Write usage data into Delta format using append mode\n",
    "3. Write tower data into Delta format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3176a836-5fc2-4d7d-9ca3-57a6a7758fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###10.1 Write customer data into Delta format using overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875c0dd1-8f28-4fc8-8056-0beba2b56b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df4.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_delta_out\",header=True,mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a610694e-8da1-4695-aa0b-76cd85f753a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###10.2 Write usage data into Delta format using append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc12351-dd3a-4230-95f2-34882619e027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df1.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usage_delta_out\",header=True,mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118dad65-5499-4d25-8035-3fc19664b34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###10.3  Write tower data into Delta format and see the output file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892a560c-aa79-4960-9ed6-db68c56e44fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mul_pth_df2.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/twr_delta_out\",header=True,\n",
    "mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "186e7415-16a8-423d-9f0b-32973ffb661a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###10.4 Read the usage data in a dataframe and show only 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc68ced-eaf2-49e4-b329-9b8e428aad67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usg_del_df=spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usage_delta_out\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d066987-bf0c-476a-9ab4-891866f69ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###10.5 Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5cc6dce-6f40-4eaa-8d82-2c179d04590a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "below is the delta file format downloaded in notepad++, this is similarDelta format is specific to databricks only and its an enriched parquet format\n",
    "\n",
    "PAR1\u0015\u0004\u0015 \u0015$\u0015´„ãÑ\u000B<\u0015\u0004\u0015 \u0012  \u0010<\u0004   5001\u0004   5004\u0015 \u0015\u0012\u0015\u0016\u0015ô˜îý\n",
    "\u001C\u0015\f\u0015\u0004\u0015\u0006\u0015\b\u001C6 (\u00045004\u0018\u00045001   \t \u0002   \u0003?\u0001\u0003*\u0015\u0004\u0015\u001C\u0015 \u0015Ç¸Ä:<\u0015\u0004\u0015 \u0012  \u000E4\u0003   101\u0003   104\u0015 \u0015\u0012\u0015\u0016\u0015ô˜îý\n",
    "\u001C\u0015\f\u0015\u0004\u0015\u0006\u0015\b\u001C6 (\u0003104\u0018\u0003101   \t \u0002   \u0003?\u0001\u0003*\u0015\u0004\u0015$\u0015(\u0015ü™Žñ\f<\u0015\u0004\u0015 \u0012  \u0012D\u0005   TWR01\u0005   TWR05\u0015 \u0015\u0012\u0015\u0016\u0015ô˜îý\n",
    "\u001C\u0015\f\u0015\u0004\u0015\u0006\u0015\b\u001C6 (\u0005TWR05\u0018\u0005TWR01   \t \u0002   \u0003?\u0001\u0003*\u0015\u0004\u0015\u001C\u0015 \u0015Æ•Ã\f<\u0015\u0004\u0015 \u0012  \u000E4\u0003   -80\u0003   -75\u0015 \u0015\u0012\u0015\u0016\u0015ô˜îý\n",
    "\u001C\u0015\f\u0015\u0004\u0015\u0006\u0015\b\u001C6 (\u0003-80\u0018\u0003-75   \t \u0002   \u0003?\u0001\u0003*\u0015\u0004\u0015\\\u0015J\u0015¸¹šƒ\u000F<\u0015\u0004\u0015 \u0012  .\\\u0013   2025-01-10 10:21:54\u0013:\u0017 \u00181:01:12\u0015 \u0015\u0012\u0015\u0016\u0015ô˜îý\n",
    "\u001C\u0015\f\u0015\u0004\u0015\u0006\u0015\b\u001C6 (\u00132025-01-10 11:01:12\u0018\u00132025-01-10 10:21:54   \t \u0002   \u0003?\u0001\u0003*\u0015\u0002\u0019lH\fspark_schema\u0015\n",
    " \u0015\f%\u0002\u0018\bevent_id% L\u001C   \u0015\f%\u0002\u0018\u000Bcustomer_id% L\u001C   \u0015\f%\u0002\u0018\btower_id% L\u001C   \u0015\f%\u0002\u0018\u000Fsignal_strength% L\u001C   \u0015\f%\u0002\u0018\ttimestamp% L\u001C   \u0016\f\u0019\u001C\u0019\\&\b\u001C\u0015\f\u00195\u0004\u0006\b\u0019\u0018\bevent_id\u0015\u0002\u0016\f\u0016¨\u0001\u0016°\u0001&T&\b\u001C6 (\u00045004\u0018\u00045001   &¸\u0001\u001C\u0015\f\u00195\u0004\u0006\b\u0019\u0018\u000Bcustomer_id\u0015\u0002\u0016\f\u0016ž\u0001\u0016¦\u0001&þ\u0001&¸\u0001\u001C6 (\u0003104\u0018\u0003101   &Þ\u0002\u001C\u0015\f\u00195\u0004\u0006\b\u0019\u0018\btower_id\u0015\u0002\u0016\f\u0016°\u0001\u0016¸\u0001&®\u0003&Þ\u0002\u001C6 (\u0005TWR05\u0018\u0005TWR01   &–\u0004\u001C\u0015\f\u00195\u0004\u0006\b\u0019\u0018\u000Fsignal_strength\u0015\u0002\u0016\f\u0016 \u0001\u0016¨\u0001&Þ\u0004&–\u0004\u001C6 (\u0003-80\u0018\u0003-75   &¾\u0005\u001C\u0015\f\u00195\u0004\u0006\b\u0019\u0018\ttimestamp\u0015\u0002\u0016\f\u0016 \u0002\u0016’\u0002&°\u0006&¾\u0005\u001C6 (\u00132025-01-10 11:01:12\u0018\u00132025-01-10 10:21:54   \u0016¶\u0007\u0016\f&\b\u0016È\u0007 \u0019\\\u0018\u0018org.apache.spark.version\u0018\u00054.0.0 \u0018)org.apache.spark.sql.parquet.row.metadata\u0018ñ\u0002{\"type\":\"struct\",\"fields\":[{\"name\":\"event_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"customer_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"tower_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"signal_strength\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"timestamp\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]} \u0018\u001Fcom.databricks.spark.jobGroupId\u0018B1765968763488_6218817062049439115_e2f298f1fc1a4c86a0ce5193c21b5a86 \u0018\u001Ecom.databricks.spark.clusterId\u0018\u00181217-105539-s25gdn23-v2n \u0018#com.databricks.spark.writeTimestamp\u0018\u001E2025-12-17T11:16:11.971238451Z \u00185parquet-mr compatible Photon version 0.2 (build 17.3) ¹\u0004  PAR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f34a43e-f290-44dd-937b-c8ae0ef133c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###10.6 Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdaa3d9e-0613-4a73-8c4d-c6fefee2bdaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Parquet file location:** \n",
    "- The data file is compressed and will be in .parquet format\n",
    "- It has the .parquet data file and other log files (started, committed)  created directly in the given path\n",
    "\n",
    "**Delta file location:**\n",
    "- The data file is compressed and will be in .parquet format\n",
    "- Data file will be created in the given path.\n",
    "- Additionally a directory called _delta_log is created in which the logs are maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95c6235-cac5-4d69-9365-185f2d490f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##11. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using saveAsTable() as a managed table\n",
    "2. Write usage data using saveAsTable() with overwrite mode\n",
    "3. Drop the managed table and verify data removal\n",
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "5. Use spark.read.sql to write some simple queries on the above tables created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfaa900-c29e-4c46-bab3-b87a1f1d5462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Write customer data using saveAsTable() as a managed table\n",
    "\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType\n",
    "cust_schema=StructType([StructField(\"customer_id\",IntegerType(),False),\n",
    "StructField(\"name\",StringType(),True),\n",
    "StructField(\"age\",IntegerType(),True),\n",
    "StructField(\"city\",StringType(),True),\n",
    "StructField(\"Plan\",StringType(),True)])\n",
    "\n",
    "cust_df1=spark.read.schema(cust_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "print(cust_df1.printSchema())\n",
    "cust_df1.show(\n",
    "\n",
    "cust_df1.write.saveAsTable(\"telecom_catalog_assign.landing_zone.cust_delta_tbl\",mode=\"overwrite\")\n",
    "display(spark.sql(\"show create table telecom_catalog_assign.landing_zone.cust_delta_tbl\"))\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.cust_delta_tbl\"))\n",
    "\n",
    "#2. Write usage data using saveAsTable() with overwrite mode\n",
    "usage_df_wr=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",header=True,inferSchema=True,sep='\\t')\n",
    "usage_df_wr.display()\n",
    "print(usage_df_wr.printSchema())\n",
    "\n",
    "usage_df_wr.write.saveAsTable(\"telecom_catalog_assign.landing_zone.usg_tbl\",mode=\"overwrite\")\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.usg_tbl\"))\n",
    "\n",
    "#3. Drop the managed table and verify data removal\n",
    "spark.sql(\"drop table telecom_catalog_assign.landing_zone.usg_tbl\")\n",
    "\n",
    "#since the table is deleted getting  below error message\n",
    "[TABLE_OR_VIEW_NOT_FOUND] The table or view `telecom_catalog_assign`.`landing_zone`.`usg_tbl` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
    "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
    "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\n",
    "                                                            \n",
    "#4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "**Table Properties**\n",
    "delta: \n",
    "enableDeletionVectors: \"true\"\n",
    "feature.appendOnly: \"supported\"\n",
    "feature.deletionVectors: \"supported\"\n",
    "feature.invariants: \"supported\"\n",
    "lastCommitTimestamp: \"1766129575000\"\n",
    "lastUpdateVersion: \"0\"\n",
    "minReaderVersion: \"3\"\n",
    "minWriterVersion: \"7\"\n",
    "spark: \n",
    "sql.statistics.auxiliaryInfo: \"{\\\"source\\\":\\\"AUTO_STATS\\\"}\"\n",
    "sql.statistics.createdAt: \"1766129573415\"\n",
    "sql.statistics.createdBy: \"root\"\n",
    "sql.statistics.numRows: \"6\"\n",
    "sql.statistics.totalSize: \"1578\"\n",
    "sql.statistics.version: \"2\"\n",
    "collation: \n",
    "collation: \"UTF8_BINARY\"\n",
    "\n",
    "#5. Use spark.read.sql to write some simple queries on the above tables create\n",
    "display(spark.sql(\"show create table telecom_catalog_assign.landing_zone.cust_delta_tbl\"))\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.cust_delta_tbl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c4cf23e-79df-4661-b068-9cf2c9309764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##12. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using insertInto() in a new table and find the behavior\n",
    "2. Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42703aa7-6f34-421a-a754-7aab66aa1063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1 Write customer data using insertInto() in a new table and find the behavior\n",
    "\n",
    "#creating new table and inserting data\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS telecom_catalog_assign.landing_zone.cust_insert_test (\n",
    "        customer_id INT,\n",
    "        name STRING,\n",
    "        age INT,\n",
    "        city STRING,\n",
    "        Plan STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "    cust_df1.write.insertInto(\"telecom_catalog_assign.landing_zone.cust_insert_test\")\n",
    "    display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.cust_insert_test\")\n",
    "    #the records got inserted successfully\n",
    "\n",
    "#2 Write usage data using insertTable() with overwrite mode\n",
    "\n",
    "#creating a DF for inserting into the cust_insert_test table with overwrite option\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType\n",
    "cust_schema=StructType([StructField(\"customer_id\",IntegerType(),False),\n",
    "StructField(\"name\",StringType(),True),\n",
    "StructField(\"age\",IntegerType(),True),\n",
    "StructField(\"city\",StringType(),True),\n",
    "StructField(\"Plan\",StringType(),True)])\n",
    "\n",
    "cust_df_wr=spark.read.schema(cust_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer1.csv\")\n",
    "print(cust_df_wr.printSchema())\n",
    "cust_df_wr.show()\n",
    "\n",
    "cust_df_wr.write.insertInto(\"telecom_catalog_assign.landing_zone.cust_insert_test\",overwrite=True)\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.cust_insert_test\"))\n",
    "\n",
    "#after running the insert into script with overwrite = True, the existing records got deleted and new records got inserted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57fa4756-0bb5-4528-9d21-b53722eb7cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##13. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data into XML format using rowTag as cust\n",
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21f228e-5a73-4d26-beaf-d684745448ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###13.1 Write customer data into XML format using rowTag as cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1594693d-0bf0-41e5-b9af-201f478a4329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Write customer data into XML format using rowTag as customer\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType\n",
    "cust_schema=StructType([StructField(\"customer_id\",IntegerType(),False),\n",
    "StructField(\"name\",StringType(),True),\n",
    "StructField(\"age\",IntegerType(),True),\n",
    "StructField(\"city\",StringType(),True),\n",
    "StructField(\"Plan\",StringType(),True)])\n",
    "\n",
    "cust_df1=spark.read.schema(cust_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "print(cust_df1.printSchema())\n",
    "cust_df1.show()\n",
    "\n",
    "cust_df1.write.xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_xml\",mode=\"overwrite\",rowTag=\"customer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c520f05f-9ad2-4648-90d3-657d38820ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###13.2 Write usage data into XML format using overwrite mode with the rowTag as usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4ae727-f5da-45e3-b9a7-93586343bc9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df_wr=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",header=True,inferSchema=True,sep='\\t')\n",
    "usage_df_wr.display()\n",
    "print(usage_df_wr.printSchema())\n",
    "\n",
    "usage_df_wr.write.xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usage_xml\",mode=\"overwrite\",rowTag=\"usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8478b2bc-81a8-4885-9f39-215e6d27047a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##14. Compare all the downloaded files (csv, json, orc, parquet, delta and xml) \n",
    "1.Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big.\n",
    "\n",
    "Using big files will give more proper size comparison\n",
    "Below is the size comparisons based on smaller file size\n",
    "\n",
    "\n",
    "| File type | File Size (KB) | Zip        |\n",
    "| :---      | :---           | :---       |\n",
    "| CSV       | 0.1484 KB      | Not zipped |\n",
    "| JSON      | 0.3213 KB      | Not zipped |\n",
    "| ORC       | 0.9033 KB      | Snappy     |\n",
    "| XML       | 0.9980 KB      | Not zipped |\n",
    "| Delta     | 1.5200 KB      | Snappy     |\n",
    "| Parquet   | 1.8000 KB      | Snappy     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1d98544-3960-44ec-ac3f-fd24a8884ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##15. Try to do permutation and combination of performing Schema Migration & Data Conversion operations like...\n",
    "1. Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format\n",
    "2. Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format\n",
    "3. Read any one of the above delta data in a dataframe and write it to dbfs in a xml format\n",
    "4. Read any one of the above delta table in a dataframe and write it to dbfs in a json format\n",
    "5. Read any one of the above delta table in a dataframe and write it to another table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06430d35-fc8a-4a18-aaab-306b65501754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format\n",
    "\n",
    "cust_orc_df1 = spark.read.orc(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_orc_out/part-00000-tid-122224858270013067-137d72c2-bb04-4033-9436-888dc84c4aa8-298-1-c000.snappy.orc\"\n",
    ")\n",
    "cust_orc_df1.toDF(\"customer_id\", \"name\", \"age\", \"city\", \"Plan\")                                                \n",
    "\n",
    "print(cust_orc_df1.printSchema())\n",
    "cust_orc_df1.display()\n",
    "\n",
    "cust_orc_df1.write.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_orc_parquet\",mode=\"overwrite\")\n",
    "spark.read.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_orc_parquet\").show()\n",
    "\n",
    "#2. Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format\n",
    "usg_prqt_df= spark.read.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_parquet_out/part-00000-tid-5588728212722270687-a3887482-9889-4b53-b3f2-82cccf982173-245-1-c000.gz.parquet\")\n",
    "usg_prqt_df.display()                                             \n",
    "\n",
    "usg_prqt_df.write.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_prqt_orc_out/\",mode=\"overwrite\")\n",
    "spark.read.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_prqt_orc_out\").display()\n",
    "\n",
    "#3. Read any one of the above delta data in a dataframe and write it to dbfs in a xml format\n",
    "\n",
    "cust_delta_df=spark.read.load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_delta_out\")\n",
    "cust_delta_df.display()\n",
    "\n",
    "cust_delta_df.write.xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_delta_xml\",rowTag=\"customer\")\n",
    "spark.read.xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/cust_delta_xml\",rowTag=\"customer\").show()\n",
    "\n",
    "#4. Read any one of the above delta table in a dataframe and write it to dbfs in a json format\n",
    "usg_del_tabl_df=spark.read.table(\"telecom_catalog_assign.landing_zone.cust_delta_tbl\")\n",
    "usg_del_tabl_df.show()\n",
    "\n",
    "usg_del_tabl_df.write.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_del1_json\",mode=\"overwrite\")\n",
    "spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/target/usg_del1_json\").display()\n",
    "\n",
    "#5. Read any one of the above delta table in a dataframe and write it to another table\n",
    "usg_del_tabl_df=spark.read.table(\"telecom_catalog_assign.landing_zone.cust_delta_tbl\")\n",
    "usg_del_tabl_df.show()\n",
    "\n",
    "usg_del_tabl_df.write.saveAsTable(\"telecom_catalog_assign.landing_zone.cust_delta_tbl_copy\",mode=\"overwrite\")\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.cust_delta_tbl_copy\"))\n",
    "display(spark.sql(\"show create table telecom_catalog_assign.landing_zone.cust_delta_tbl_copy\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee7a2181-e9ef-4c6e-9245-efed9ae9c854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##16. Do a final exercise of defining one/two liner of... \n",
    "1. When to use/benifits csv\n",
    "2. When to use/benifits json\n",
    "3. When to use/benifit orc\n",
    "4. When to use/benifit parquet\n",
    "5. When to use/benifit delta\n",
    "6. When to use/benifit xml\n",
    "7. When to use/benifit delta tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d10c99a-64f0-4f94-9950-64ad4276117b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1. When to use/benifits csv\n",
    "\n",
    "2. When to use/benifits json\n",
    "3. When to use/benifit orc\n",
    "4. When to use/benifit parquet\n",
    "5. When to use/benifit delta\n",
    "6. When to use/benifit xml\n",
    "7. When to use/benifit delta tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f3ee5f3-d12d-4249-9969-a547e6a92aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5923107162383115,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read_write_usecases",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
