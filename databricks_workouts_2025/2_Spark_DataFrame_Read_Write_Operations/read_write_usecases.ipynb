{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba86a20-5a3a-4130-86f5-e312f4a7901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain ReadOps Assignment\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2766646-7223-43da-9293-5805c95f9e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign;\n",
    "create schema if not exists telecom_catalog_assign.landing_zone ;\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf2fe69e-4278-47a2-a034-23ca08dcfe82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93eac91c-06a6-455c-87f3-79ef2e4e16b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### a. Volume vs DBFS \n",
    "\n",
    "**Volumes** are Unity Catalog-governed objects that provide a logical layer for managing non-tabular data (files) within cloud object storage with centralized governance. They are the modern, recommended approach in Databricks and support access control policies and lineage tracking through Unity Catalog.  \n",
    "**DBFS/FileStore** (Databricks File System) is an older abstraction over cloud storage that allowed users to interact with data using simple paths or mounts without robust, centralized governance. The DBFS root and its mounts are deprecated, and Databricks recommends migrating to volumes or external locations under Unity Catalog.\n",
    "\n",
    "\n",
    "| Feature                | Volumes (Unity Catalog)                                                                 | DBFS/FileStore (Deprecated)                                  |\n",
    "|------------------------|----------------------------------------------------------------------------------------|--------------------------------------------------------------|\n",
    "| Governance             | Centralized, fine-grained access control via Unity Catalog                             | Limited, workspace-level controls                            |\n",
    "| Data Lineage           | Supported                                                                              | Not supported                                                |\n",
    "| Recommended for Prod   | Yes                                                                                    | No                                                           |\n",
    "| Access Control         | Unity Catalog policies                                                                 | ACLs, less granular                                          |\n",
    "| Usage                  | Non-tabular data, files, ML models, etc.                                               | General file storage                                         |\n",
    "| Migration              | Modern, recommended approach                                                           | Deprecated, migrate to Volumes or External Locations         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e0fbc6-648e-4f4b-a2a4-54967df480ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###b. Why production teams prefer Volumes for regulated data\n",
    "\n",
    "\n",
    "Production teams prefer Volumes for regulated data primarily due to their integration with Unity Catalog, which provides a robust framework for data governance, security, and access management. \n",
    "\n",
    "**Centralized Governance and Auditing:** Unity Catalog provides a single place to manage data access, permissions, and auditing across all Databricks workspaces. This is essential for meeting compliance requirements for regulated data.\n",
    "\n",
    "**Granular Access Control:** Volumes allow administrators to define precise access controls on specific volumes or subfolders within the cloud storage location, which is critical for restricted and sensitive data.\n",
    "\n",
    "**Simplified Compliance:** The structured, governed approach of volumes simplifies compliance reporting and ensures data is handled consistently according to organizational policies and industry regulations.\n",
    "\n",
    "**Lifecycle Management:** Volumes have their own policies for permissions, encryption, backup, and recovery, which aids in managing the full data lifecycle in a compliant manner. \n",
    "\n",
    "**DBFS** lacks these centralized governance features, making it difficult to enforce consistent, auditable access controls required for sensitive or regulated production data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d239288e-3a41-4de6-8658-9090955691b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Loading the datasets into Volume\n",
    "\n",
    "1. Write code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3654af-8ef0-4034-a2b9-dc9a58027772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Loading the customer data into customer.csv file\n",
    "\n",
    "customer_csv = ''' 101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,\n",
    "104,Raj,52,Mumbai,\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID '''\n",
    "dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",customer_csv,overwrite=True)\n",
    "\n",
    "#Loading the usage data into usage.tsv file\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",usage_tsv,overwrite=True)\n",
    "\n",
    "#Loading the tower log data into regionwise .tsv file\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''\n",
    "dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1/tower_logs_region1.csv\",tower_logs_region1,overwrite=True)\n",
    "dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2/tower_logs_region2.csv\",tower_logs_region1,overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb2dca0-cb14-414d-8bd1-f404539f2208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4dafe9-d3dd-48b6-914b-c7c25340af20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Verify if data is loaded\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82b08008-0c45-444a-a227-35dee2f5f1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#validating if customer file is loaded successfully\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.head(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e461bd-882f-4823-9606-3bfd79178745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#validating if usage file is loaded successfully\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.head(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afed1170-cce6-4b0b-abd8-91797d1267c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#validating if tower log file is loaded successfully\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1/\")\n",
    "dbutils.fs.head(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1/tower_logs_region1.csv\")\n",
    "\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2/\")\n",
    "dbutils.fs.head(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2/tower_logs_region2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Directory Read Use Cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "542af18c-e005-45fe-97cf-c8c8832fd96c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.1. Read all tower logs using: Path glob filter (example: *.csv) Multiple paths input Recursive lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9ec7fd-96b0-48cd-8572-f16c91d462cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pth_glb_fl_df1=spark.read.csv(path=[\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1\",\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2\"],header=True,inferSchema=True,sep='|',pathGlobFilter=\"*.csv\",recursiveFileLookup=True)\n",
    "pth_glb_fl_df1.show()\n",
    "display(pth_glb_fl_df1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ad9629-481f-4fe0-8355-e8c5a3d05f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d49c48d-bd54-448c-8b2b-9fd6fad90bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using pathGlobFilter\n",
    "\n",
    "pth_glb_fl_df1=spark.read.csv(path=[\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1\",\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2\"],header=True,inferSchema=True,sep='|',pathGlobFilter=\"*.csv\",recursiveFileLookup=True)\n",
    "pth_glb_fl_df1.show()\n",
    "display(pth_glb_fl_df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5116f156-897b-41c8-ae11-e68663548ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using list of paths in spark.read.csv([path1, path2]) \n",
    "\n",
    "mul_pth_df1=spark.read.csv(path:=[\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower1/\",\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower2/\"],header=True,inferSchema=True, sep='|',recursiveFileLookup=True)\n",
    "mul_pth_df1.show()\n",
    "display(mul_pth_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b997b351-1f41-41d1-b838-5a270f009329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "mul_pth_df2= spark.read.option(\"header\",True).option(\"delimiter\",\"|\").option(\"recursiveFileLookup\",True).format(\"csv\").load([\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\"])\n",
    "mul_pth_df2.show()\n",
    "display(mul_pth_df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b96d73d-aff7-41e7-bcc9-446f45f3c140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.3 Compare the outputs and understand when each should be used.\n",
    "\n",
    "- pathGlobFilter=\"*.csv\" -This can be used to read files of specific(*.csv) format within the specifed path/folder\n",
    "- List of paths -This can be used to read files from multiple paths/sources\n",
    "- Option -This can be used for more than one options, and parameters can be passed to the option & recursiveFileLookup can be used to read files from all the sub folders in the mentioned path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1ee4c77-28d0-46d8-8774-78b7cfe1eb89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Read cust file with option and header=true, inferSchema=true\n",
    "cust_df1=spark.read.option(\"header\",True).option(\"inferSchema\",True).option(\"delimiter\",\",\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df1)\n",
    "print(cust_df1.schema)\n",
    "\n",
    "##Read cust file with options and header=true, inferSchema=true\n",
    "cust_df2=spark.read.options(header=True,inferSchema=True,delimiter=\",\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df2)\n",
    "print(cust_df2.schema)\n",
    "\n",
    "#Read cust file with options.csv and header=true, inferSchema=true\n",
    "cust_df3=spark.read.options(header=True,inferSchema=True,delimiter=\",\").csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df3)\n",
    "print(cust_df3.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9617a90-17c3-4380-bd58-7c93f177411d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##Read cust file with option and header=false, inferSchema=false\n",
    "cust_df4=spark.read.option(\"header\",False).option(\"inferSchema\",False).option(\"delimiter\",\",\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df4)\n",
    "print(cust_df4.schema)\n",
    "\n",
    "##Read cust file with options and header=false, inferSchema=false\n",
    "cust_df5=spark.read.options(header=False,inferSchema=False,delimiter=\",\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df5)\n",
    "print(cust_df5.schema)\n",
    "\n",
    "#Read cust file with options.csv and header=False, inferSchema=false\n",
    "cust_df6=spark.read.options(header=False,inferSchema=False,delimiter=\",\").csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(cust_df6)\n",
    "print(cust_df6.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c19db0-24f9-4500-a035-03908873cc6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Read usage file with option and header=True, inferSchema=True\n",
    "usage_df1=spark.read.option(\"header\",True).option(\"inferSchema\",True).option(\"delimiter\",\"\\t\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df1)\n",
    "print(usage_df1.schema)\n",
    "\n",
    "##Read usage file with options and header=True, inferSchema=True\n",
    "usage_df2=spark.read.options(header=True,inferSchema=True,delimiter=\"\\t\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df2)\n",
    "print(usage_df2.schema)\n",
    "\n",
    "##Read usage file with options.csv and header=True, inferSchema=True\n",
    "usage_df3=spark.read.options(header=True,inferSchema=True,delimiter=\"\\t\").csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df3)\n",
    "print(usage_df3.schema) \n",
    "\n",
    "##Read usage file with option and header=False, inferSchema=False\n",
    "usage_df4=spark.read.option(\"header\",False).option(\"inferSchema\",False).option(\"delimiter\",\"\\t\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df4)\n",
    "print(usage_df4.schema)\n",
    "\n",
    "##Read usage file with options and header=False, inferSchema=False\n",
    "usage_df5=spark.read.options(header=False,inferSchema=False,delimiter=\"\\t\").format(\"csv\").load(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df5)\n",
    "print(usage_df5.schema)\n",
    "\n",
    "##Read usage file with options.csv and header=False, inferSchema=False\n",
    "usage_df6=spark.read.options(header=False,inferSchema=False,delimiter=\"\\t\").csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "display(usage_df6)\n",
    "print(usage_df6.schema) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb76bc66-b4e3-4e10-9f47-a92c6cc90d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write a note on What changed when we use header or inferSchema with true/false?\n",
    "\n",
    "- **Header = True** - When header is assigned as True its considering the first row as header\n",
    "- **Header = False** - When header is assigned as False its assigning header as c0, c1, c2..etc.,\n",
    "- **InferSchema = True** - When inferSchema is assigned as True it scans all rows and assigns datatype accordingly. It can be used in small datasets, but in huge volume of data full scan takes time and needs to be used carefully. Sampling ratio option can be used to avoid full scanning.\n",
    "- **InferSchema = False** -When inferSchema is assigned as False all the columns are treated as strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67afb6a0-951a-4bbe-835a-c6e645401a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How schema inference handled “abc” in age?\n",
    "\n",
    "The column age has one value as \"abc\", even when InferSchema is True its assigned as string datatype, to be on safer side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ff42c77-a068-4302-a496-ffbba95ddce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5.1 Apply column names using string using toDF function for customer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "120611e3-a3ff-465a-92e9-e052814c065d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df=spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",header=True,inferSchema=True).toDF(\"customer_id\",\"name\",\"age\",\"city\",\"plan\")\n",
    "display(cust_df)\n",
    "print(cust_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae348339-125a-4424-b097-616151443410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5.2 Apply column names and datatype using the schema function for usage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47d7d6e-9ead-4f3d-afd6-498b5c9772cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "str_struct= \"customer_id integer,voice_mins integer,data_mb integer,sms_count integer\"\n",
    "usage_df=spark.read.schema(str_struct).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",header=True,sep=\"\\t\")\n",
    "display(usage_df)\n",
    "print(usage_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d68caf16-4bc5-4289-9ebd-5777cc5afee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5.3 Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b68da42-3b86-4dab-ad1b-82a4e97127ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"id\",IntegerType(),False),StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"event_id\",IntegerType(),True),StructField(\"customer_id\",IntegerType(),True),StructField(\"tower_id\",StringType(),True),StructField(\"signal_strength\",IntegerType(),True),StructField(\"timestamp\",StringType())])\n",
    "usage_df_df=spark.read.schema(custom_schema).csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/sourcedata/custs_header_1\")\n",
    "print(usage_df.printSchema())\n",
    "usage_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1d6d88-7bcc-4548-a0d1-15d37f6fc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. More to come (stay motivated)...."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5923107162383115,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read_write_usecases",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
