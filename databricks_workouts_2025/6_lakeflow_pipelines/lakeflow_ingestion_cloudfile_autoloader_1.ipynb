{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60b2d07-3e25-4359-9cf9-81099e048779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/infoblisstech@gmail.com/databricks-code-repo/6_lakeflow_pipelines/autoloader_file_ingestion_usecase1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85287e4-ec7b-4427-9d2f-2448171aa7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Auto Loader is Databricks\n",
    "**Auto Loader is Databricksâ€™ cloud-native file ingestion engine for ingesting new files incrementally from object storage.**\n",
    "\n",
    "Supported Sources:\n",
    "- AWS S3\n",
    "- Azure ADLS Gen2\n",
    "- Google Cloud Storage (GCS)\n",
    "\n",
    "Modes:\n",
    "- **Directory listing** - Directory listing scans storage paths to detect new files (This works in free edition)\n",
    "- File notification - Processes files as soon as they arrive at scale (This will not work in free edition because the cloud storage event trigger can't control/trigger Databricks LF Ingestion)\n",
    "\n",
    "**Directory listing** (Databricks Lakeflow Ingestion - Autoloader - Directory Listing)\n",
    "1. Spark lists the Cloud directory (pull model)\n",
    "2. Detects new files **(Incremental Autoloader)**\n",
    "3. Infers schema / **evolves** if needed\n",
    "4. Copy the file(s) & store the schema info in a schema file, so further schema inference is not needed.\n",
    "5. After file1 is copied to Bronze layer -> Updates checkpoint (maintaining the file info of whichever is copied already)\n",
    "6. Waits for next trigger of the Lakeflow pipeline and follow step 1 to 5.\n",
    "\n",
    "**File Notification** (we will see it in the cloud databricks version)\n",
    "1. Cloud storage emits file-create event (S3 Event, ADLS Event Grid, GCS Pub/Sub)\n",
    "2. Event is delivered to Databricks queue\n",
    "3. Auto Loader receives notification (push model)\n",
    "4. New file is registered\n",
    "5. Infers schema / evolves if needed\n",
    "6. Copy the file(s) & store the schema info in a schema file, so further schema inference is not needed.\n",
    "7. Updates checkpoint (file1 is processed...)\n",
    "8. Stream stays idle until next event arrives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af3cdcb-192b-449d-a62b-e92b014cbd56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Benifits of Autoloader:**\n",
    "- Incremental and Efficient File Ingestion: Auto Loader automatically detects and processes new files as they arrive in your source directory (e.g., S3 or Unity Catalog volume). This eliminates manual tracking and reprocessing, ensuring only new data is ingested each run.\n",
    "\n",
    "- Schema Evolution Support: With options like \"cloudFiles.schemaEvolutionMode\": \"addNewColumns\" and \"mergeSchema\": \"true\", Auto Loader can handle changes in your data schema over time, adding new columns without breaking your pipeline.\n",
    "\n",
    "- Scalability and Resource Optimization: Properties such as \"cloudFiles.maxFilesPerTrigger\" allow you to control how many files are processed per batch, helping manage resource usage and scale to large datasets.\n",
    "\n",
    "- Checkpointing and Fault Tolerance: Auto Loader maintains checkpoints and schema locations, so it can resume from where it left off in case of failures, ensuring reliable and consistent data ingestion.\n",
    "\n",
    "- Unified Streaming and Batch Processing: By using readStream and writeStream, your pipeline can handle both streaming and batch workloads seamlessly, making it suitable for real-time and scheduled data ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01b7923-a4d8-4f3d-80dd-5c4ef09add16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**To perform schema evolution, we have to use the below properties:**<br>\n",
    "**Read side:** <br>\n",
    ".option(\"cloudFiles.schemaEvolutionMode\",\"addNewColumns\")<br>\n",
    "**Write side:** <br>\n",
    ".option(\"mergeSchema\", \"true\")<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfcc37fa-f40e-4216-99f9-e3cb83d1fec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We learn Autoloading of Incremental data from cloud source, Schema evolution, \n",
    "cloudsrc=\"/Volumes/catalog2_we47/schema2_we47/datalake/sourcepath/\"#s3 storage path\n",
    "#cloudsrc=\"gs://izsourcebucket/Master_City_List_hour1.csv\"\n",
    "bronzetgt=\"/Volumes/catalog2_we47/schema2_we47/datalake/bronze/streamwrite1/\"\n",
    "#To resolve it, you must use a data source that is accessible from your AWS-based Databricks workspace, such as an S3 bucket or a Unity Catalog volume.\n",
    "ckptlocation=\"/Volumes/catalog2_we47/schema2_we47/datalake/bronze/streamwrite1/_checkpoint\"#stores the files copied information post write is successful\n",
    "schemalocation=\"/Volumes/catalog2_we47/schema2_we47/datalake/bronze/streamwrite1/_schema\"#stores the inferred schema of the source data\n",
    "df1=spark.readStream.format(\"cloudFiles\")\\\n",
    ".option(\"cloudFiles.format\",\"csv\")\\\n",
    ".option(\"cloudFiles.maxFilesPerTrigger\",1)\\\n",
    ".option(\"cloudFiles.inferColumnTypes\",True)\\\n",
    ".option(\"cloudFiles.schemaEvolutionMode\",\"addNewColumns\")\\\n",
    ".option(\"checkpointLocation\", ckptlocation)\\\n",
    ".option(\"cloudFiles.schemaLocation\", schemalocation)\\\n",
    ".option(\"header\",True)\\\n",
    ".load(cloudsrc)#this can be s3/adls/gcs\n",
    "#.option(\"cloudFiles.useNotifications\", \"true\") (Remove this option to enable directory listing)\n",
    "#maxFilesPerTrigger - this property help spark to process howmany files in an iteration to control the resource utilization (all files will be processed ultimately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4429e5-657c-4430-866d-166242c6ec80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "#realtime trigger is not possible in free serverless\n",
    "#writeStream will read data from df1 (materialized here) and write to bronzetgt using the schema generated by reader and checkpoint info stored\n",
    "df1.writeStream.trigger(availableNow=True)\\\n",
    ".option(\"checkpointLocation\", ckptlocation)\\\n",
    ".option(\"cloudFiles.schemaLocation\", schemalocation)\\\n",
    ".option(\"mergeSchema\", \"true\") \\\n",
    ".start(bronzetgt)\n",
    "#.option(\"mergeSchema\", \"true\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17256ab-d5c4-4176-86b9-77aaee7dace5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(bronzetgt).orderBy(\"city_name\").show(100)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7723175997136475,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lakeflow_ingestion_cloudfile_autoloader_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
