{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c60b2d07-3e25-4359-9cf9-81099e048779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/infoblisstech@gmail.com/databricks-code-repo/6_lakeflow_pipelines/autoloader_file_ingestion_usecase1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d85287e4-ec7b-4427-9d2f-2448171aa7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Auto Loader** is Databricks’ cloud-native file ingestion engine for ingesting new files incrementally from object storage.\n",
    "\n",
    "Supported Sources:\n",
    "- AWS S3\n",
    "- Azure ADLS Gen2\n",
    "- Google Cloud Storage (GCS)\n",
    "\n",
    "Modes:\n",
    "- **Directory listing** - Directory listing scans storage paths to detect new files using the checkpoint feature\n",
    "- File notification - Processes files as soon as they arrive at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccacfddc-7395-455f-9aa1-e1b4c604434f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Directory listing\n",
    "1. Spark lists directory\n",
    "2. Detects new CSV or any files\n",
    "3. Infers schema / evolves if needed\n",
    "4. store the file into bronze layer\n",
    "5. Updates checkpoint (file1 is processed...)\n",
    "6. Waits for next job run to list directory and collect any new files into bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfcc37fa-f40e-4216-99f9-e3cb83d1fec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.readStream.format(\"cloudFiles\")\\\n",
    ".option(\"cloudFiles.format\",\"csv\")\\\n",
    ".option(\"cloudFiles.schemaEvolutionMode\",\"addNewColumns\")\\\n",
    ".option(\"cloudFiles.maxFilesPerTrigger\",5)\\\n",
    ".option(\"cloudFiles.inferColumnTypes\",True)\\\n",
    ".option(\"checkpointLocation\", \"/Volumes/catalog2_we47/schema2_we47/datalake/bronze/streamwrite1/_checkpoint\")\\\n",
    ".option(\"cloudFiles.schemaLocation\", \"/Volumes/catalog2_we47/schema2_we47/datalake/bronze/streamwrite1/_schema\")\\\n",
    ".option(\"header\",True)\\\n",
    ".load(f\"/Volumes/catalog2_we47/schema2_we47/datalake/sourcepath/\")#this can be s3/adls/gcs\n",
    "#.option(\"cloudFiles.useNotifications\", \"true\") (Remove this option to enable directory listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4429e5-657c-4430-866d-166242c6ec80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "df1.writeStream.trigger(availableNow=True)\\\n",
    ".option(\"mergeSchema\", \"true\")\\\n",
    ".option(\"checkpointLocation\", \"/Volumes/catalog2_we47/schema2_we47/datalake/bronze/streamwrite1/_checkpoint\")\\\n",
    ".option(\"cloudFiles.schemaLocation\", \"/Volumes/catalog2_we47/schema2_we47/datalake/bronze/streamwrite1/_schema\")\\\n",
    ".start(f\"/Volumes/catalog2_we47/schema2_we47/datalake/bronze/streamwrite1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17256ab-d5c4-4176-86b9-77aaee7dace5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(\"/Volumes/catalog2_we47/schema2_we47/datalake/bronze/streamwrite1/\").orderBy(\"shipment_id\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d2fee02-eabe-4ca2-8014-0d54ae5bdd00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Debugging in Databricks has been challenging for me in my current project, and I’m still learning the most effective approach. I’m trying to understand one of the existing pipelines, as it uses Structured Streaming, CDF. Currently, I’m adding show() at different stages in a feature branch to observe how the DataFrame transforms. Is this the right approach, or is there a better way to debug and understand the flow?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "old_lakeflow_ingestion_cloudfile_autoloader_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
