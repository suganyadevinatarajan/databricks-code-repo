{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d7f53f-32c8-40ba-982d-04c7548932b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Simply say- We are going to learn...\n",
    "next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a76be2-1041-4fc6-b94b-ab46b78e169c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9435574f-1e19-4ddd-acc4-b5f11c5fcd5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Passive Data Munging** - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "\n",
    "**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc009f1a-5864-4b7b-8697-1dd72790eb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging - \n",
    "- Visible - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b76842c2-34fc-43a7-8138-e331de2af1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality \n",
    "- - Null columns are there\n",
    "- - duplicate rows\n",
    "- - format issues are there (age is not in number format eg. 7-7)\n",
    "- - Uniformity issues (Artist, artist)\n",
    "- - Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- - Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88ccadc2-69ed-4744-8673-b83a29cd0eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b7e978-9f46-4506-8ab2-c79f8bcf1089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "#rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))\n",
    "display(rawdf1.sample(.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870e142d-523c-49a3-8c70-1cbd96d00377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "rawdf1.printSchema()#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(rawdf1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(rawdf1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "for i in rawdf1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0])\n",
    "\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9ed1b9-96c1-4f58-939a-320911e29b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ccb7e0e-aeec-4789-aecd-61187cacd34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bb4d001-0ab5-4975-8792-2d575385b193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62bbe034-7274-45e6-ae89-9f83243aeea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. how to create pipelines using different data processing techniques by connecting with different sources/targets\n",
    "2. how to Standardize/Modernization/Industrializing the code and how create/consume generic/reusable functions & frameworks\n",
    "3. Testing (Unit, Peer Review, SIT/Integration, Regression, User Acceptance Testing), Masking engine,\n",
    "4. Reusable transformation(munge_data, optimize_performance),\n",
    "5. Quality suite/Data Profiling/Audit engine (Reconcilation) (Audit framework), Data/process Observability\n",
    "\n",
    "6. how terminologies/architecture/submit jobs/monitor/log analysis/packaging and deployment ...\n",
    "7. performance tuning\n",
    "8. Deploying spark applications in Cloud & other Distributions like Hortonworks/Cloudera/Databricks\n",
    "9. Creating cloud pipelines using spark SQL programs & Cloud native tools\n",
    "\n",
    "What is the importance of learning this program or How this can address interview questions..?\n",
    "VERY VERY IMPORTANT PROGRAM IN TERMS OF EXPLAINING/SOLVING PROBLEMS GIVEN IN INTERVIEW ,\n",
    "WITH THIS ONE PROGRAM YOU CAN COVER ALMOST ALL DATAENGINEERING FEATURES\n",
    "Tell me about the common transformations you performed,\n",
    "tell me your daily roles in DE,\n",
    "tell me some business logics you have writtened recently\n",
    "How do you write an entire spark application,\n",
    "levels/stages of DE pipelines or\n",
    "have you created DE pipelines what are the transformations applied,\n",
    "how many you have created or are you using existing framework or you created some framework?\n",
    "\n",
    "'''\n",
    "TRANSFORMATION & ANALYTICAL TECHNIQUES\n",
    "Starting point - (Data Governance (security) - Tagging, categorization, classification, masking/filteration)\n",
    "1. Data Munging - Process of transforming and mapping data from Raw form into Tidy(usable) format with the\n",
    "intent of making it more appropriate and valuable for a variety of downstream purposes such for\n",
    "further Transformation/Enrichment, Egress/Outbound, analytics, model application & Reporting\n",
    "a. Passive - Data Discovery EDA (Exploratory Data Analytics)\n",
    "(every layers ingestion/transformation/analytics/consumption) -\n",
    "Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns.\n",
    "b. Active - Combining Data + Schema Evolution/Merging (Structuring)\n",
    "c. Validation, Cleansing, Scrubbing - Identifying and filling gaps & Cleaning data to remove outliers and inaccuracies\n",
    "Preprocessing, Preparation\n",
    "Cleansing (removal of unwanted datasets eg. na.drop),\n",
    "Scrubbing (convert of raw to tidy na.fill or na.replace),\n",
    "d. Standardization, De Duplication and Replacement & Deletion of Data to make it in a usable format (Dataengineers/consumers)\n",
    "\n",
    "2. Data Enrichment - Makes your data rich and detailed\n",
    "a. Add, Remove, Rename, Modify/replace\n",
    "b. split, merge/Concat\n",
    "c. Type Casting, format & Schema Migration\n",
    "\n",
    "3. Data Customization & Processing - Application of Tailored Business specific Rules\n",
    "a. User Defined Functions\n",
    "b. Building of Frameworks & Reusable Functions\n",
    "\n",
    "4. Data Curation\n",
    "a. Curation/Transformation\n",
    "b. Analysis/Analytics & Summarization -> filter, transformation, Grouping, Aggregation/Summarization\n",
    "\n",
    "5. Data Wrangling - Gathering, Enriching and Transfomation of pre processed data into usable data\n",
    "a. Lookup/Reference\n",
    "b. Enrichment\n",
    "c. Joins\n",
    "d. Sorting\n",
    "e. Windowing, Statistical & Analytical processing\n",
    "f. Set Operation\n",
    "\n",
    "6. Data Publishing & Consumption - Enablement of the Cleansed, transformed and analysed data as a Data Product.\n",
    "a. Discovery,\n",
    "b. Outbound/Egress,\n",
    "c. Reports/exports\n",
    "d. Schema migration\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d9b849-607e-4852-9687-b41332caa867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_End_End_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
