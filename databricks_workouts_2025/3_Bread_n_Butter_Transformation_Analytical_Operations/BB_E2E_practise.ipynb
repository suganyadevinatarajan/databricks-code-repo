{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ce7620-3be4-4c67-97e7-46e09a953c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#importing spark session\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ShortType, LongType, DoubleType,FloatType,DateType,BooleanType,TimestampType\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql.functions import lit,initcap,col,initcap\n",
    "\n",
    "struct=StructType([StructField('id',StringType(), True),StructField('firstname',StringType(), True),StructField('lastname',StringType(), True),StructField('age',StringType(), True),StructField('profession',StringType(), True)])\n",
    "\n",
    "#reading the csv file and converting it into spark dataframe adding schema\n",
    "rawdf1=spark.read.schema(struct).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custsmodified\",sep=',',mode=\"permissive\")\n",
    "rawdf1.show(20)\n",
    "rawdf1.count()\n",
    "\n",
    "#----Active Data Munging----\n",
    "\n",
    "#cleansing\n",
    "#na.drop will remove the rows which has null values in all the columns, if subset is specified then records having nulls in the specified columns will be removed\n",
    "cleanseddf1=rawdf1.na.drop(how='all',subset=[\"firstname\",\"lastname\"])\n",
    "display(cleanseddf1.count())\n",
    "\n",
    "#scrubbing\n",
    "\n",
    "#na.fill will fill the nulls with provided values in specified coulmns\n",
    "scrubbeddf1=cleanseddf1.na.fill(\"not provided\",subset=[\"lastname\",\"profession\"])\n",
    "\n",
    "#na.replace will replace a value with another in given subset\n",
    "scrubbeddf2=scrubbeddf1.na.replace(\"not provided\",\"NA\",subset=[\"lastname\"])\n",
    "repl_dict_list={\"Actor\":\"Celebrity\",\"Pilot\":\"Captain\"}\n",
    "scrubbeddf3=scrubbeddf2.na.replace(repl_dict_list,subset=[\"profession\"])\n",
    "display(scrubbeddf3.show(10))\n",
    "\n",
    "#De-duplication\n",
    "\n",
    "#removing row level duplicate using distinct\n",
    "dedupdf1=scrubbeddf3.distinct()\n",
    "display(dedupdf1.show(10))\n",
    "\n",
    "\n",
    "#removing column level duplicate using dropDuplicates\n",
    "dedupdf2=dedupdf1.coalesce(1).dropDuplicates(['id']) #coalesce is used to reduce the number of partitions to 1, so that all records will be brought into 1 partition and then duplicates will be removed, this is recommended only for very less number of records not for huge records, since it will cause performance issue\n",
    "display(dedupdf2.show(10))\n",
    "\n",
    "#display(dedupdf1.where(\"id in ('4000003')\"))\n",
    "#below will dlete the duplicate records prioritised on id and retain the record with highest age for the id=4000003, using order by\n",
    "#dedupdf1.coalesce(1).where(\"id in('4000003')\").orderBy([\"id\",\"age\"], ascending=[True,False]).show()\n",
    "#dedupdf1.coalesce(1).where(\"id in('4000003')\").orderBy([\"id\",\"age\"], ascending=[True,False]).dropDuplicates(['id']).show()\n",
    "\n",
    "#Data Standardization\n",
    "\n",
    "#Standardization 1 - Column Enrichment\n",
    "stddf1=dedupdf2.withColumn(\"sourcesystem\",lit(\"source\"))\n",
    "display(stddf1.show(10))\n",
    "\n",
    "#standardization 2 - Column Uniformity\n",
    "stddf2 =stddf1.withColumn(\"profession\",initcap(stddf1.profession))\n",
    "display(stddf2.show(10))\n",
    "\n",
    "#standardization 3 - Column Transformation\n",
    "\n",
    "#stddf2.select(col(\"age\")).distinct().show()\n",
    "#stddf2.select(col(\"id\")).distinct().orderBy(\"id\").show()\n",
    "\n",
    "#rlike is regular expression like function that help us identify any string data in our DF column\n",
    "#stddf2.where(\"id like 't%'\").show()\n",
    "#stddf2.where(\"id rlike 'a-z'\").show()\n",
    "#stddf2.where(\"id rlike 'A-Z'\").show()\n",
    "stddf2.where(\"id rlike '[a-zA-Z]'\").show()\n",
    "#stddf2.where(\"id rlike '[A-Z]'\").show()\n",
    "stddf2.where(\"age rlike '[^0-9]'\").show() #checking for any non number values in age column\n",
    "#stddf2.where(\"age rlike '0-9'\").show() #checking for any number values in age column\n",
    "\n",
    "#stddf3 = stddf2.withColumn(\"id\",replace(col(\"id\"),\"ten\",\"10\")) #to replace only one value\n",
    "#display(stddf3.where (\"firstname = 'Elsie'\")) -#to check if id is updated successfully\n",
    "repl_list={\"one\":\"1\",\"two\":\"2\",\"three\":\"3\",\"four\":\"4\",\"five\":\"5\",\"six\":\"6\",\"seven\":\"7\",\"eight\":\"8\",\"nine\":\"9\",\"ten\":\"10\"}\n",
    "stdf3 = stddf2.na.replace(repl_list,subset=[\"id\"]) #created dictonary to replace list of \n",
    "stddf3 = stdf3.withColumn(\"age\",regexp_replace(\"age\",\"-\",\"\")) #using regular expression replace function to replace 7-7 with 77\n",
    "stdf3.where(\"id rlike '[a-zA-Z]'\").show()\n",
    "stddf3.where(\"age rlike '[^0-9]'\").show() \n",
    "\n",
    "#standardization 4 - Datatype Standardization\n",
    "stddf4=stddf3.withColumn(\"id\", stddf3.id.cast(IntegerType())).withColumn(\"age\", stddf3.age.cast(ShortType()))\n",
    "stddf4.printSchema()\n",
    "\n",
    "#standardization 5 - ColumnRename\n",
    "stddf5=stddf4.withColumnsRenamed({\"id\":\"custid\",\"sourcesystem\":\"srcsys\"})\n",
    "display(stddf5.show(10))\n",
    "\n",
    "#standardization 6 - ReorderColumn\n",
    "stddf6=stddf5.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"srcsys\")\n",
    "mungeddf=stddf6\n",
    "display(mungeddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6603d6f5-c176-42ef-9fd2-f3a12fb01590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DataEnrichment\n",
    "#performing EDA on munged data\n",
    "mungeddf.printSchema()\n",
    "display(mungeddf.take(20))\n",
    "display(\"total rows\",len(mungeddf.collect()))\n",
    "display(mungeddf.summary())\n",
    "\n",
    "mungeddf.take(20)\n",
    "mungeddf.show(20)\n",
    "#mungeddf.collect()\n",
    "#mungeddf.count()\n",
    "\n",
    "# adding date columns\n",
    "enrichdf1=mungeddf.withColumns({\"datadt\":lit(\"25/30/12\"),\"loaddt\":current_date()})\n",
    "display(enrichdf1)\n",
    "#or\n",
    "enrichdf1=mungeddf.selectExpr(\"*\",\" '25/30/12' as datadt\",\"current_date() as loaddt\")\n",
    "#or\n",
    "enrichdf1=mungeddf.select(\"*\",lit('25/30/12').alias('datadt'),current_date().alias('loaddt'))\n",
    "enrichdf1.printSchema()\n",
    "\n",
    "#derive columns\n",
    "enrichdf2=enrichdf1.withColumn(\"profflag\",substring(\"profession\",1,1))\n",
    "#or\n",
    "enrichdf2=enrichdf1.select(\"*\",substring(\"profession\",1,1).alias(\"profflag\"))\n",
    "#or\n",
    "enrichdf2=enrichdf1.selectExpr(\"*\",\"substring(profession,1,1) as profflag\")\n",
    "display(enrichdf2)\n",
    "\n",
    "#Rename of columns\n",
    "enrichdf3=enrichdf2.withColumnsRenamed({\"profflag\":\"professionflag\",\"srcsys\":\"sourcesystem\"})\n",
    "display(enrichdf3)\n",
    "\n",
    "#modify/replace columns\n",
    "enrichdf4=enrichdf3.withColumn(\"profession\",col(\"professionflag\"))\n",
    "#or\n",
    "enrichdf4=enrichdf3.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",concat(\"profession\",lit(\"-\"),\"professionflag\").alias(\"profession_flag\"),\"sourcesystem\",\"datadt\",\"loaddt\")\n",
    "display(enrichdf4)\n",
    "\n",
    "#remove column\n",
    "enrichdf5=enrichdf4.drop(\"profession_flag\")\n",
    "display(enrichdf5)\n",
    "\n",
    "#splitting data\n",
    "splitdf=enrichdf4.withColumn(\"profession\",split(\"profession_flag\",'-'))\n",
    "splitdf=splitdf.withColumn(\"profession_flag\",col(\"profession\")[1])\n",
    "splitdf=splitdf.withColumn(\"profession\",col(\"profession\")[0])\n",
    "display(splitdf)\n",
    "#or\n",
    "splitdf1=enrichdf4.withColumn(\"shortprof\",upper(substring(\"profession\",1,3).alias(\"prof\"))).withColumn(\"profession_flag\",split(\"profession_flag\",'-')).withColumn(\"profession_flag\",col(\"profession_flag\")[1])\n",
    "display(splitdf1)\n",
    "\n",
    "#merging of column\n",
    "mergeddf=splitdf1.withColumn(\"fullname\",concat(\"firstname\",lit(\" \"),\"lastname\"))\n",
    "display(mergeddf)\n",
    "#or\n",
    "mergeddf=splitdf1.select(\"custid\",\"age\",concat(\"firstname\",lit(\" \"),\"lastname\").alias(\"fullname\"),\"profession\",\"profession_flag\",\"shortprof\",\"sourcesystem\",\"datadt\",\"loaddt\")\n",
    "display(mergeddf)\n",
    "\n",
    "#column formatting/type casting\n",
    "formatteddf=mergeddf.withColumn(\"datadt\",to_date(\"datadt\",\"yy/dd/mm\"))#25/30/12 -> 2025-12-30\n",
    "display(formatteddf)\n",
    "formatteddf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5f2545-c140-4998-95de-bf1ec7aff873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#how to write a python program to append a variable value to another variable and use it inside the selectExpr\n",
    "name='suganya'\n",
    "sqlexpression=f\"'{name}' as owner\"\n",
    "print(sqlexpression)\n",
    "mungeddf.selectExpr(\"*\",sqlexpression).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60257e4-ec19-47f6-971a-6d5276ba31e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data Customization\n",
    "\n",
    "#defining a UDF - user defined spark ready function, since there is no predefined spark ready function available to perform your operation\n",
    "#Create Python Custom Function with complex logics\n",
    "\n",
    "#Calculating age category from the given age of the customer\n",
    "def agecat(age):\n",
    "    if age is None:\n",
    "        return \"Unknown\"\n",
    "    elif age<=10:\n",
    "        return \"child\"\n",
    "    elif age>10 and age<=18:\n",
    "        return \"teenager\"\n",
    "    elif age>18 and age<=30:\n",
    "        return \"young\"\n",
    "    elif age>30 and age<=50:\n",
    "        return \"middleaged\"\n",
    "    else:\n",
    "        return \"senior\"\n",
    "    \n",
    "print(agecat(10))\n",
    "    \n",
    "from pyspark.sql.functions import udf\n",
    "udfagecat=udf(agecat) #promoting a python function to userdefined spar ready function\n",
    "customdf=formatteddf.withColumn(\"agecat\",udfagecat(\"age\"))\n",
    "display(customdf.take(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4ba4b10-f80f-42b8-b7d3-d428edfd040d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data Curation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_E2E_practise",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
