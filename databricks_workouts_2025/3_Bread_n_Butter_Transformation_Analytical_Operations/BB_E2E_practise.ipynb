{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ed9eb1-9041-4d1a-b4b8-686a2bfcbbe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reading the csv file and converting it into spark dataframe adding schema\n",
    "rawdf1=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custsmodified\",header=False,sep=',').toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "rawdf1.show(20)\n",
    "print(\"actual Record count\",rawdf1.count())\n",
    "rawdf1.printSchema()\n",
    "rawdf1.describe()\n",
    "print(\"Columns of rawdf1\",rawdf1.columns)\n",
    "print(rawdf1.dtypes)\n",
    "\n",
    "rawdf1.summary()\n",
    "#to view schema of rawdf1\n",
    "print(rawdf1.schema)\n",
    "\n",
    "#removing row level duplicates using distinct\n",
    "print(\"distinct records in rawdf1\",rawdf1.distinct().count())\n",
    "\n",
    "#distinct & dropduplicates performs similar operations only, parameters cant be passed in distinct, but parameters can be passed in dropDuplicates\n",
    "print(\"deduplication of records in rawdf1\",rawdf1.dropDuplicates().count())\n",
    "\n",
    "#removing column level duplicates using dropDuplicates by passing parameters\n",
    "print(\"distinct records in rawdf1 after removing column level duplicates\",rawdf1.dropDuplicates(['id']).count())\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())\n",
    "len(rawdf1.collect())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ce7620-3be4-4c67-97e7-46e09a953c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#importing spark session\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ShortType,\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "struct=StructType([StructField('id',StringType(), True),StructField('firstname',StringType(), True),StructField('lastname',StringType(), True),StructField('age',StringType(), True),StructField('profession',StringType(), True)])\n",
    "\n",
    "#reading the csv file and converting it into spark dataframe adding schema\n",
    "rawdf1=spark.read.schema(struct).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custsmodified\",sep=',',mode=\"permissive\")\n",
    "rawdf1.show(20)\n",
    "rawdf1.count()\n",
    "\n",
    "#----Active Data Munging----\n",
    "\n",
    "#cleansing\n",
    "#na.drop will remove the rows which has null values in all the columns, if subset is specified then records having nulls in the specified columns will be removed\n",
    "cleanseddf1=rawdf1.na.drop(how='all',subset=[\"firstname\",\"lastname\"])\n",
    "display(cleanseddf1.count())\n",
    "\n",
    "#scrubbing\n",
    "\n",
    "#na.fill will fill the nulls with provided values in specified coulmns\n",
    "scrubbeddf1=cleanseddf1.na.fill(\"not provided\",subset=[\"lastname\",\"profession\"])\n",
    "\n",
    "#na.replace will replace a value with another in given subset\n",
    "scrubbeddf2=scrubbeddf1.na.replace(\"not provided\",\"NA\",subset=[\"lastname\"])\n",
    "repl_dict_list={\"Actor\":\"Celebrity\",\"Pilot\":\"Captain\"}\n",
    "scrubbeddf3=scrubbeddf2.na.replace(repl_dict_list,subset=[\"profession\"])\n",
    "display(scrubbeddf3.show(10))\n",
    "\n",
    "#De-duplication\n",
    "\n",
    "#removing row level duplicate using distinct\n",
    "dedupdf1=scrubbeddf3.distinct()\n",
    "display(dedupdf1.show(10))\n",
    "\n",
    "\n",
    "#removing column level duplicate using dropDuplicates\n",
    "dedupdf2=dedupdf1.coalesce(1).dropDuplicates(['id'])\n",
    "display(dedupdf2.show(10))\n",
    "display(dedupdf2.where(\"id in ('4000001')\"))\n",
    "\n",
    "#Data Standardization\n",
    "\n",
    "#Standardization 1 - Column Enrichment\n",
    "stddf1=dedupdf2.withColumn(\"sourcesystem\",lit(\"source\"))\n",
    "display(stddf1.show(10))\n",
    "\n",
    "#standardization 2 - Column Transformation\n",
    "stddf2 =stddf1.withColumn(\"profession\",initcap(stddf1.profession))\n",
    "display(stddf2.show(10))\n",
    "\n",
    "display(stddf1.groupBy(\"profession\").count())#DSL"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_E2E_practise",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
